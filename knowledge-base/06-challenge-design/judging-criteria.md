# Judging Criteria

**Last Updated:** December 2024

---

## Overview

All Hack for RVA submissions are evaluated by a panel of judges using consistent criteria. This document defines how projects are scored and winners selected.

---

## Scoring Rubric

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Impact** | 30% | Would this meaningfully improve Richmond? |
| **Feasibility** | 25% | Can this realistically be built and deployed? |
| **Innovation** | 20% | Is this a creative or novel approach? |
| **Presentation** | 15% | Is the solution clearly explained? |
| **Civic Alignment** | 10% | Does it address the challenge and pillar goals? |

---

## Detailed Criteria

### Impact (30%)

*Does this solution address a real need and create meaningful change?*

| Score | Description |
|-------|-------------|
| 5 | Transformative impact on a significant civic problem |
| 4 | Strong impact with clear beneficiaries identified |
| 3 | Moderate impact, addresses a real but limited need |
| 2 | Limited impact, nice-to-have but not essential |
| 1 | Minimal impact, unclear how this helps Richmond |

Questions judges consider:
- How many Richmonders would benefit?
- How significant is the problem being solved?
- Is there evidence of real need?

---

### Feasibility (25%)

*Can this actually be built and deployed?*

| Score | Description |
|-------|-------------|
| 5 | Fully functional prototype, clear path to production |
| 4 | Working demo with minor gaps, realistic roadmap |
| 3 | Proof of concept, significant work remaining |
| 2 | Basic mockup, many technical uncertainties |
| 1 | Conceptual only, unclear if buildable |

Questions judges consider:
- Does the demo work?
- Is the technology appropriate?
- Are there clear next steps for implementation?
- Could the city actually adopt this?

---

### Innovation (20%)

*Is this a fresh approach to the problem?*

| Score | Description |
|-------|-------------|
| 5 | Highly original approach, novel use of technology |
| 4 | Creative solution with unique elements |
| 3 | Solid approach with some innovative aspects |
| 2 | Straightforward implementation, limited innovation |
| 1 | Derivative or obvious solution |

Questions judges consider:
- Have we seen this before?
- Does it combine ideas in a new way?
- Is the technology use creative?

---

### Presentation (15%)

*Can the team clearly communicate their solution?*

| Score | Description |
|-------|-------------|
| 5 | Compelling pitch, clear demo, polished materials |
| 4 | Clear explanation, good demo, minor rough edges |
| 3 | Adequate presentation, understandable but not memorable |
| 2 | Confusing explanation or poor demo quality |
| 1 | Unable to clearly explain what the solution does |

Questions judges consider:
- Can I understand what this does in 2 minutes?
- Is the demo convincing?
- Did the team answer questions well?

---

### Civic Alignment (10%)

*Does this address the challenge and pillar goals?*

| Score | Description |
|-------|-------------|
| 5 | Directly addresses challenge, strong pillar alignment |
| 4 | Clear connection to challenge with minor deviations |
| 3 | Related to challenge but somewhat tangential |
| 2 | Weak connection to assigned pillar |
| 1 | Does not address the civic challenge |

Questions judges consider:
- Does this solve the stated problem?
- Would the pillar stakeholders want this?
- Is this what the city asked for?

---

## Prize Categories

### Grand Prize ($5,000)
- Highest overall score across all criteria
- Must meet minimum threshold in each criterion

### Pillar Prizes ($1,000+ each)
- Best solution in each of the 7 pillars
- Judged by pillar stakeholder panel

### Special Categories (amounts vary)
- **Best First-Time Hackers** — Team of first-time participants
- **Best Design** — Outstanding UX/UI
- **Best Use of Data** — Creative data analysis
- **People's Choice** — Voted by attendees
- **Most Likely to Implement** — City department endorsement

---

## Judging Process

### Saturday Night
- Submissions due 11:59 PM
- Initial review by organizers for completeness

### Sunday Morning
- Judges review all submissions (video + documentation)
- Score using rubric
- Identify finalists for live demos

### Sunday Afternoon
- Finalist teams present live (3-5 minutes + Q&A)
- Final scoring and deliberation
- Winners announced at Awards Ceremony

---

## Judge Panel

Each pillar has a panel including:
- City department representative
- Nonprofit partner representative
- Technical expert
- Corporate sponsor representative (optional)

Grand Prize judged by cross-pillar panel including:
- Mayor's office representative
- Event Director
- Cross-sector leaders

---

## Disqualification

Teams may be disqualified for:
- Submitting after deadline
- Violating Code of Conduct
- Using prohibited pre-built solutions
- Plagiarism or misrepresentation

---

## Related Documents

- [Challenge Design README](README.md) — Challenge framework
- [MAP Alignment](map-alignment.md) — Pillar details
- [Code of Conduct](../09-legal-compliance/code-of-conduct.md) — Expected behavior

---

*Source: knowledge-base/06-challenge-design/judging-criteria.md*
